# Abstract

LLMs, as language models, are known for their ability to generalize across various natural 
language tasks. Existing works [1-3] have leveraged their world knowledge and pattern-matching abilities to perform 
time series forecasting by incorporating adapters for numerical input and projection heads for output. However, there 
has been little exploration into whether LLMs, with the addition of adapters or proper fine-tuning, can perform general 
time series tasks—such as imputation, classification, and outlier detection—without task-specific fine-tuning. 

# Team - Vaibhav Malviya (UCLA ECE MS Student)

# Submissions

* [Proposal](proposal)
* [Midterm Checkpoint Presentation Slides](https://docs.google.com/presentation/d/10kWvrLXxfz-bzX5cdHgTzIwtaByFaYedC41DMM_jEi8/edit#slide=id.p)
* [Final Presentation Slides](http://)
* [Final Report](report)
