# LLMs for Sensor Time Series Data

# Abstract

LLMs, as Large language models, are recognized for their ability to generalize across a broad range of natural language tasks. Existing research has harnessed their extensive world knowledge and powerful pattern-recognition capabilities, often through the integration of specialized adapters and fine tuning for numerical input and projection layers for output generation. However, there remains limited exploration into whether LLMs can effectively handle more general time series tasks, either with or without task-specific fine-tuning applied to raw time series data or image-transformed representations. This study aims to assess LLM performance on edge devices by evaluating four different configurations: fine-tuning on raw data, no fine-tuning on raw data, fine-tuning on image-transformed data, and no fine-tuning on image-transformed data.


# Contributor:
* Vaibhav Malviya (UCLA ECE MS Student)
* Mentor - Pengrui Quan (UCLA ECE PhD)

# Submissions

* [Proposal](proposal)
* [Midterm Checkpoint Presentation Slides](https://docs.google.com/presentation/d/10kWvrLXxfz-bzX5cdHgTzIwtaByFaYedC41DMM_jEi8/edit?usp=sharing)
* [Final Presentation Slides](https://docs.google.com/presentation/d/1_SklIq-xxRoEaixQl1hIpkUYRqFZInTHj3LqIhaVpz4/edit?usp=sharing)
* [Final Report](report)
